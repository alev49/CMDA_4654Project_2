---
title: "CMDA-4654  \n Homework Template"
subtitle: "Group Exercise 2"
author: "Alex Vidal and Seth Eshraghi"
date: "March 16, 2024"
output:
  pdf_document:
    highlight: haddock
keep_tex: no
number_sections: no
html_document:
  df_print: paged
geometry: margin = 0.5in
header-includes:
- \usepackage{booktabs}
- \usepackage{longtable}
- \usepackage{array}
- \usepackage{multirow}
- \usepackage{wrapfig}
- \usepackage{float}
- \usepackage{colortbl}
- \usepackage{pdflscape}
- \usepackage{tabu}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage[normalem]{ulem}
- \usepackage{makecell}
- \usepackage{xcolor}
editor_options:
  chunk_output_type: console
documentclass: article
urlcolor: blue
---

<!-- The above is set to automatically compile to a .pdf file.   -->

<!-- It will only succeed if LaTeX is installed. -->

<!-- If you absolutely can't get LaTeX installed and/or working, then you can compile to a .html first,  -->

<!-- by clicking on the arrow button next to knit and selecting Knit to HTML. -->

<!-- You must then print you .html file to a .pdf by using first opening it in a web browser and then printing to a .pdf -->

```{r setup, include=FALSE}
# This is the setup chunk
#  Here you can set global options for the entire document

library(knitr) # I recommend doing this here

# Although you can call functions from a library using the following notation
#  without loading the entire library.
knitr::opts_chunk$set(echo = TRUE, 
                      comment = NA, # Required
                      fig.path = "./figures/",  # Store all figures here in relative path (make the folder first)
                      fig.align = "center",
                      fig.width = 7,
                      fig.height = 7,
                      message = FALSE, # Turn off load messages
                      warning = FALSE # Turn off warnings
                      )

```

\clearpage

```{r include=FALSE}
# You should not echo this chunk.
# include=FALSE does more than echo=FALSE, it actually does: echo=FALSE, results='hide', message=FALSE, warning=FALSE, fig.show='hide'

# You should set your working directory at the very beginning of your R Markdown file
setwd("/Users/alexvidal/Desktop/CMDA_4654/")

```


# Part 1

Testing loess
```{r}
#data from https://www.itl.nist.gov/div898/handbook/pmd/section1/dep/dep144.htm 
testData <- read.csv("exData.csv", header = FALSE,col.names = c("x", "y")) 
model <- loess(y~x,data=testData, span = 1/3,degree = 2,family = "symmetric")
model$fitted
```


## Function
```{r}
library(ggplot2)
# Gets a single x point and finds n closest points to xPt in points along with their corresponding weights
# Returns a data frame of those n closest to xPt points with columns x, y and weight 
# points is a data frame with columns x and y and N rows so n<N
minPoint <- function(xPt, points, n){
  closestX <- c()
  closestY <- c()
  weight <- c()
  
  #Gets the first n smallest distances
  minDists <- sort((abs(xPt-points$x)))[1:n] 
  
  #Stores max dist
  maxDist <- max(minDists)
  
  #Standardizes the smallest distances so largest dist is now 1
  minDists <- minDists/maxDist
  
  # Goes through all the points and compares their distances
  # to see if it is in the smallest distances list above
  for(index in 1:nrow(points)){
    distBetween <- abs(xPt-points$x[index])/maxDist
    if(distBetween  %in% minDists){
      closestX <- c(closestX, points$x[index])
      closestY <- c(closestY, points$y[index])
      
      #Removes smallest distance from our list
      minDists <- minDists[-match(distBetween, minDists)]
      
      # Basically largest distance point gets wieght of 0
      if(distBetween>=1){
        weight <- c(weight, 0)
      }
      
      #Otherwise plug distance in the weight formula below
      # Using the tricube wight function for this 
      else{
        weight <- c(weight, (1-distBetween^3)^3)
      }
    }
  }
  
  #Creates the final data frame with the top n closest points with
  #corresponding weights
  closest <- data.frame(closestX, closestY, weight)
  return(closest)
}


# Your function will have the following inputs.
# 
# * x - a numeric input vector
# * y - a numeric response
#
# Note span and degree are shown with their default values. (Read about this in the description)
# * degree should be 1 or 2 only
# * span can be any value in interval (0, 1) non-inclusive.
#
# If show.plot = TRUE then you must show a plot of either the final fit

myloess <- function(x, y, span = 0.5, degree = 1, show.plot = TRUE){
  
  #Total number of data points
  N <- length(x)
  
  #Creating a data frame for x and y points
  points <- data.frame(x,y) 
  
  # Checks if span is within usable parameters
  if(span>=1 | span<=0){
    return("Span needs to be within 0 and 1 exclusive")
  }
  
  #True loess fit according to built in R function
  truFitVals <- predict(loess(y~x, data=points, span = span, degree = degree))
  truData <- data.frame(x=x, y=truFitVals)
  
  # The regression values
  regValues <- c()
  
  # Loops through the data set using its index and calculates each 
  # points n=(span*N) closest points and those points weights
  # Plugs that new data into linear model with corresponding 
  # degrees and weights and stores their predicted values at the same point. 
  for(index in 1:N){
    
    # Closest points data obtained form our function above
    pointData <- minPoint(points$x[index], points, round(span*N))
    
    # Fitting data to linear model with degree 1 and stores the predicted value
    # in regValues
    if(degree==1){
      pointModel <- lm(closestY~closestX, data = pointData, weights = pointData$weight)
      regValues <- c(regValues, pointModel$coefficients[1]+pointModel$coefficients[2]*points$x[index])
    }
    
    # Same but with degree 2
    else if(degree==2){
      
      #Creating the 2nd parameter for our 2 degree polynomial
      pointData$closestXSquared <- pointData$closestX^2
      
      pointModel <- lm(closestY~closestX+closestXSquared, data = pointData, weights = pointData$weight)
      regValues <- c(regValues, pointModel$coefficients[1]+pointModel$coefficients[2]*points$x[index]+
                       pointModel$coefficients[3]*(points$x[index])^2)
    }
    
    #Catches non 1 and 2 degree values
    else{
      return("Error: Degree too large")
    }
  }
  
  # Calculates Sum of Squares Error
  SSE <- sum((points$y-regValues)^2)
  
  # Our new fitted data based on 
  fittedData <- data.frame(x=points$x, y=regValues)
  # The plot that shows my LOESS function and R built in LOESS function
  thePlot <- ggplot()+
    geom_point(data = points,aes(x=x, y=y))+
    geom_line(data = fittedData, aes(x=x, y=y)+
    labs(title=paste(" LOESS Plot with", degree, "Degree Polynomial", "\n", "with Span:", span, sep = " "),
        x ="x variable", y = "y variable")
  
  # Compiles all the results in list
  result <- list(Span=span, degree=degree, N_total=N, SSE=SSE, loessplot=thePlot)
  
  # Plots the plot if true otherwise returns our result
  if(show.plot){
    plot(thePlot)
  }
  
  return(result) 
}

# Your function should return a named list containing the following:
# span: proportion of data used in each window (controls the bandwidth)
# degree: degree of polynomial
# N_total: total number of points in the data set
# SSE: Error Sum of Squares (Tells us how good of a fit we had).
# loessplot: An object containing the ggplot so that we can see the plot later. 
#  We want this even if show.plot = FALSE
#  Note: you are NOT allowed to simply use stat_smooth() or geom_smooth() to have it automatically do LOESS.
#  You should use geom_line() or similar to plot your final the LOESS curve.

# Make sure you can access the objects properly using the $ notation.
```


## Problem 1
```{r}
load("ozone.RData")
head(ozone)
```

### 1.
```{r}
for(i in 1:6){
  fit <- lm(ozone~poly(temperature, i), data = ozone)
  print(paste("Sum of Squares Residual for",i ,"degree polynomial is:",sum(fit$residuals^2), sep = " "))
  print(summary(fit))
}
```

I would say that based on the adjusted R squared the sweet spot for the model to have the least amount of parameters with the best fit would be 4 degree polynomial. Since the adjusted R-squared for degree 4 is the highest as even though the regular R-squared keeps going up for the 5 adn 6 degree polynomials, it is only barely and we are adding more unnecessary parameters. 

### 2. 
```{r}
SSETable <- matrix(1, ncol=11, nrow = 2,byrow=TRUE)
plotTable <- matrix(1, ncol=11, nrow = 2,byrow=TRUE)
for(j in 1:2){
  index <- 1
  for(i in seq(.25,.75, by=.05)){
    fit <- myloess(ozone$ozone, ozone$temperature, span=i, degree = j, show.plot = FALSE)
    SSETable[j, index] <- fit$SSE
    index <- index+1
  }
}
colnames(SSETable) <- c(seq(.25,.75, by=.05))
rownames(SSETable) <- c(1,2)

print("Sum of Squares Error Table")
print(SSETable)

print("Best Fitted Plots by Polynomial Degree")
spans <- seq(.25,.75, by=.05)
for(i in 1:2){
  for (j in 1:3){
    fit <- myloess(ozone$ozone, ozone$temperature, span=spans[j], degree = i)
  }
}
```

All the best plots for both degree 1 and degree 2 had the lowest spans since they were probably over fitting in some areas especially as seen in the degree 2 span .25 plot. In the beginning the line varies greatly and is not as smooth as I would like to be used in modeling. Degree 2 span of .35 actually looks much smoother and if you look at the SSE Table it is not that much difference in SSE compared to .3. For degree 1 even with span of .25 it's SSE is still not lower than degree 2 span of .35 but looks smoother than it. Overall degree 2 span of .25 and .3 look very over fitting and even span of .35 does a bit while degree 1 span of .25 is slight over fitting with the other spans looking pretty smooth. 

## Problem 2
```{r}
library(MASS)
data("mcycle")
```

### 1.
```{r}
SSETable <- matrix(1, ncol=11, nrow = 2,byrow=TRUE)
plotTable <- matrix(1, ncol=11, nrow = 2,byrow=TRUE)
for(j in 1:2){
  index <- 1
  for(i in seq(.25,.75, by=.05)){
    fit <- myloess(mcycle$times, mcycle$accel, span=i, degree = j, show.plot = FALSE)
    SSETable[j, index] <- fit$SSE
    index <- index+1
  }
}

colnames(SSETable) <- c(seq(.25,.75, by=.05))
rownames(SSETable) <- c(1,2)

print("Sum of Squares Error Table")
print(SSETable)

bestSpans <- c(.25,.3,.35)
for(i in 1:2){
  for (j in 1:3){
    fit <- myloess(mcycle$times, mcycle$accel, span=bestSpans[j], degree = i)
  }
}
```

Based on visuals the best looking fit for the data looks to be in the degree 2 polynomial fit as degree 1 fit are just too smooth and dampened so they don't look that well fit. For teh degree 2 span the SSE for .25 says it is the best but .3 and .35 look the smoothest with some decrease in SSE. 

# Part 2

## Function
```{r}
library(caret)
# Your function will have the following inputs similar to what you would find with the
#  knn() function
#
# * train - matrix or data frame of training set cases
# * test - matrix or data frame of test set cases.  
#     (A vector will be interpreted as a row vector for a single case.)
# * y_train - Either a numeric vector, or factor vector for the responses in the training set
# * y_test - Either a numeric vector, or factor vector for the responses in the testing set
# * k - number of neighbors considered, the default value is 3
#
# If weighted = TRUE, then your function must used the distance weighted kNN as described above,
#  otherwise it should do the default knn method.

# Specific function that finds the k nearest neighbors 
# from the newPt and the surrounding other points
# Will return a  sorted  data frame of smallest to largest weight
# and the index of k nearest points
findNeighbors <- function(newPt, points, k){
  
  neighbors <- data.frame(matrix(ncol = 2, nrow = k))
  
  colnames(neighbors) <- c("index", "weight")
  
  #Creates empty data frame with 2 columns
  distances <- data.frame(matrix(ncol=2,nrow=0))
  colnames(distances) <- c("Distance", "Index")
  if(!is.list(points)){
    points <- data.frame(points)
  }
  n <- nrow(points)
  #Loops through all of the points and calculates the euclidean distance
  #for each point to the newPt. Stores it in the data frame
  for(p in 1:n){
    
    #Adds new row to data frame and stores the value in it.
    if(is.list(points)){
      d <- dist(x = rbind(points[p, ], newPt), method="euclidean")
      distances[nrow(distances)+1,] <- c(Distance=d, Index=p)
    }
    else{
      d <- dist(x = rbind(points[p], newPt), method="euclidean")
      distances[nrow(distances)+1,] <- c(Distance=d, Index=p)
    }
    
  }
  #Stores the first k indices and weights
  #sorted smallest to largest.
  neighbors$index <- distances$Index[order(distances$Distance)][1:k]
  w <-c()
  
  #Standardize the weights so even if distance is 
  # 0 it gets the highest weight of 1 while 
  # other larger distances get  weights closer to 0
  for(i in sort(distances$Distance)[1:k]){ 
    w <- c(w, (1/exp(i)))
  }
  neighbors$weight <- w
  return(neighbors)
}

mykNN <- function(train, test, y_train, y_test, k = 3, weighted = TRUE){
  
  #Sets if we have to perform a regression or not
  regression <- FALSE
  
  #Checks if testing and training responses are numeric
  if(is.numeric(y_train) | is.numeric(y_test)){
    regression <- TRUE
  }
  
  #Makes sure if doing classification set the classifiers as factors
  else{
    y_train <- as.factor(y_train)
    y_test <- as.factor(y_test)
  }
  
  #Will be our predictions for both regression and classification
  classified <- c()
  
  #Checks if we have 1 dimensional training/testing data and transforms
  # it into data frame to make standardize how we access and store our data
  if(!is.list(test)){
    test <- data.frame(test)
    train <- data.frame(train)
  }
  
  #Just sets the indices we gonna loop thru
  n <- nrow(test)
  
  # Loop through all the training points and find each respective
  # k neighbors and classify the point based on highest number of 
  # classifications within the k neighbors and store it 
  for(point in 1:n){
    
    #Finds the k nearest neighbor's indices
    neighbors <- findNeighbors(test[point, ], train, k)
    
    #For weighted classification
    if(weighted & !regression){
      
      #Adds new column the classification column to the neighbors data frame
      neighbors$class <- y_train[neighbors$index]
      
      #Finds all of the unique classifications
      uniqueClass <- unique(neighbors$class)
      
      #Creates new data frame where we gonna store total weight for 
      #each unique classification
      totalWeights <- c()
      totalWeights <- data.frame(matrix(ncol = 2, nrow = 0))
      colnames(totalWeights) <- c("Total_Weight", "Classification")
      
      #Loops through all the unique classifications, calculates the total weight for each
      # unique classification and stores it in our new data frame totalWeights
      for(c in uniqueClass){
        total <- sum(neighbors$weight[neighbors$class==c])
        totalWeights[nrow(totalWeights)+1, ] <- data.frame(Total_Weight=total, Classification=c)
      }
      
      #Stores the unique classification with the highest total weight
      classified <- c(classified,totalWeights$Classification[totalWeights$Total_Weight==max(totalWeights$Total_Weight)])
      
      
    } 
    
    #For non weighted classification
    else if (!weighted & !regression){
      
      #Adds new column the classification column to the neighbors data frame
      neighbors$class <- y_train[neighbors$index]
      
      #Finds largest occurrence of a classification and stores it
      classified <- c(classified, names(table(neighbors$class))[which.max(table(neighbors$class))])
    }
    
    #For the weighted regression
    else if(weighted & regression){
      
      #Calculates the c to give each response variable a weight
      theC <- neighbors$weight/sum(neighbors$weight)
      
      #Store our predicted value which in regression would be the yhat
      classified <- c(classified, sum(theC*y_train[neighbors$index]))
      
    }
    
    else{
      
      classified <- c(classified, mean(y_train[neighbors$index]))
                      
    }
    
    
  }
  
  #On to calculating the residuals errors and accuracy of our classification
  if(!regression){
    
    # Creates confusion Matrix that shows for the predicted 
    # classifications versus our correct test set
    factors <- unique(y_test)
    y_test <- factor(y_test,levels = factors)
    classified <-factor(classified, levels = factors)
    confMat <- confusionMatrix(data = classified, reference = y_test)
    acc <- confMat$overall[1]
    resultClass <- list(Predicted_Responses=classified, accuracy=acc, Error_Rate=1-acc, Confusion_Matrix=confMat$table, Value_of_k=k)
    return(resultClass)
  }
  
  
  else{
    resid <- classified-y_test
    sse <- sum(resid^2)
    resultReg <- list(Predicted_Responses=classified, Residual_Vector=resid, SSE=sse, Value_of_k=k)
    return(resultReg)
  }
  
}

# If you are doing classification, then your function must return:
#  * A factor vector (yhat) for the predicted categories for the testing data
#  * The accuracy of the classification
#  * The error rate = 1 - accuracy
#  * A confusion matrix
#  * The value of k used

# If you are doing regression, then your function must return:
#  * A numeric vector (yhat) for the predicted responses for the testing data
#  * The residual vector
#  * The SSE
#  * The value of k used
```

## Problem 3
```{r}
# Some pre-processing
library(ISLR)
# Remove the name of the car model and change the origin to categorical with actual name
Auto_new <- Auto[, -9]
# Lookup table
newOrigin <- c("USA", "European", "Japanese")
Auto_new$origin <- factor(newOrigin[Auto_new$origin], newOrigin)

# Look at the first 6 observations to see the final version
head(Auto_new)
```

### 1. 
```{r}
set.seed(21)
dt <- (sample(nrow(Auto_new), nrow(Auto_new)*.7))
trainData <- Auto_new[dt, -8]
testData <- Auto_new[-dt, -8]
y_test <- Auto_new[-dt, 8]
y_train <- Auto_new[dt, 8]
```

### 2. 
```{r}
kNNs <-c()
for(k in seq(2,20)){
  kNNs <- c(kNNs, mykNN(train=trainData, test = testData, y_test = y_test, y_train = y_train, k = k, weighted = FALSE))

}
accuracy <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(accuracy) <- c("K_Value", "Accuracy")
for(i in seq(2,length(kNNs),5)){
  accuracy[nrow(accuracy)+1, ] <- c(kNNs[i+3],kNNs[i])
}
```

### 3. 
```{r}
knitr::kable(accuracy)
```

### 4. 
```{r}
ggplot(data = accuracy,aes(x=K_Value, y= Accuracy))+
  geom_line()+
  labs(title=("Plot of k Vlaues vs Accuracy Regular kNN Method"))
```
Best k value is probably k=6

### 5.
```{r}
kNNs[4*6]
```

```{r}
knn5Reg <- mykNN(train=trainData$mpg, test = testData$mpg, y_test = testData$weight, y_train = trainData$weight, k = 5, weighted = FALSE)
knn10Reg <- mykNN(train=trainData$mpg, test = testData$mpg, y_test = testData$weight, y_train = trainData$weight, k = 10, weighted = FALSE)

k5data <- data.frame(cbind(knn5Reg$Predicted_Responses, testData$mpg, testData$weight))
ggplot(data = k5data)+
  geom_point(aes(x=X2, y=X1, colour="Predicted Values"),alpha=.5, colour='red')+
  geom_point(aes(x=X2, y=X3, colour="Actual Values"), colour='blue', shape=5)+
  labs(title=("Plot for k=5 value \nmpg vs. weight"), x="Miles Per Gallon", y="Weight", colour="Legend")

k10data <- data.frame(cbind(knn10Reg$Predicted_Responses, testData$mpg, testData$weight))
ggplot(data = k10data)+
  geom_point(aes(x=X2, y=X1, colour="Actual Values"),alpha=.5, colour='red')+
  geom_point(aes(x=X2, y=X3, colour="Predicted Values"), colour='blue', shape=5)+
  labs(title=("Plot for k=10 value \nmpg vs. weight"), x="Miles Per Gallon", y="Weight")
```


## Problem 4
```{r}
load("ozone.RData")
head(ozone)
dt <- (sample(nrow(ozone), nrow(ozone)*0.631))
trainData <- ozone[dt, 3]
testData <- ozone[-dt, 3]
y_test <- ozone[-dt, 1]
y_train <- ozone[dt, 1]
plot(trainData,y_train)
```

### Part a.
```{r}
kNNs <-c()
kvalues=c(1,3,5,10,20)
for(k in kvalues){
  kNNs <- c(kNNs, mykNN(train=trainData, test = testData, y_test = y_test, y_train = y_train, k = k, weighted = TRUE))
}

kNNs
```

### Part b.
```{r}
trainData <- ozone[dt, -1]
testData <- ozone[-dt, -1]
y_test <- ozone[-dt, 1]
y_train <- ozone[dt, 1]

kNNs <-c()
for(k in 1:20){
  kNNs <- c(kNNs, mykNN(train=trainData, test = testData, y_test = y_test, y_train = y_train, k = k, weighted = TRUE))
}
kNNs
```





